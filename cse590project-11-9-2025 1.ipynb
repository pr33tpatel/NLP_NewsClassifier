{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8c26cb6",
   "metadata": {},
   "source": [
    "# CSE 590 Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7d22dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following is for creating embedding datasets for training pytorch NN's for multi-classification\n",
    "from sentence_transformers import SentenceTransformer      # We use this to create datasets with the text already embedded as dense vectors\n",
    "import torch                                               # For training neural networks on the data...\n",
    "import numpy as np\n",
    "\n",
    "# The following is for fine-tuning GPT-2 for classification task\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Other helper modules\n",
    "import random\n",
    "from tqdm import tqdm # use this for getting embeddings with batching and the loaders later\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88514af7",
   "metadata": {},
   "source": [
    "## Define classes and functions here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffb355e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These can eventually go into the utils.py!\n",
    "##############################################################################\n",
    "#         General Purpose Functions \n",
    "##############################################################################\n",
    "def set_seed(seed): # referencing supplemental materials from Andriy Burkov's \"The Hundred-page Language Models Book\"\n",
    "    \"\"\"\n",
    "    Set all seeds to have reproducibility!\n",
    "\n",
    "    Args:\n",
    "        seed := Integer seed value for random number generation\n",
    "    \"\"\"\n",
    "    \n",
    "    random.seed(seed)                   # Python's built-in random seed\n",
    "    torch.manual_seed(seed)             # CPU random seed\n",
    "    torch.cuda.manual_seed_all(seed)    # GPU random seed\n",
    "\n",
    "    \n",
    "    # Request cuDNN to use deterministic algorithms\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    # Disable cuDNN's auto-tuner for consistent behavior\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def set_hyperparameters(): # referencing Andriy Burkov's \"The Hundred-page Language Models Book\"\n",
    "    \"\"\"\n",
    "    Simple function to return the typical hyperparameters such as number of epochs, batch size and learning rate\n",
    "    \"\"\"\n",
    "    num_epochs=1 # previous fine-tuned model used 3 epochs\n",
    "    batch_size=32\n",
    "    learning_rate=2e-5\n",
    "    return num_epochs, batch_size, learning_rate\n",
    "     \n",
    "\n",
    "def get_df(which_df):\n",
    "    '''\n",
    "        A function to get one of our preprocessed datasets. Everything should expect a pandas DataFrame named 'df_complete'\n",
    "\n",
    "    '''\n",
    "    if which_df == 'pulk17':\n",
    "    #######################################\n",
    "    #           PRE-PROCESS DATA (BINARY CLASSIFICATION)\n",
    "    #######################################\n",
    "\n",
    "        df = pd.read_csv(\"hf://datasets/Pulk17/Fake-News-Detection-dataset/train.tsv\", sep=\"\\t\")\n",
    "        #df['text'] = 'A news story dated to ' + df['date'] + ' has the title ' + '\"' + df[\"title\"] + '\"' + ' The subject is ' + df['subject'] +' and says the following: ' + df[\"text\"].astype(str)\n",
    "        print(df.head(5))\n",
    "        print(df['text'][0])\n",
    "\n",
    "        df['split'] = None # initialize this now, set it after we get our splits' indices\n",
    "\n",
    "        # Create train / test / valid split, then create our set of embedding datasets, and likewise prepare it for GPT-2 classification\n",
    "        np.random.seed(42)  # for reproducibility\n",
    "        indices = np.arange(len(df))\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        # split sizes, here we do 90% train, 5% test, 5% valid\n",
    "        n_total = len(df)\n",
    "        n_train = int(0.8 * n_total)\n",
    "        n_test = int(0.1 * n_total)\n",
    "        #n_valid = n_total - n_train - n_test  # handles rounding\n",
    "\n",
    "        # splits are by indexing, and we also assign to the split col\n",
    "        train_idx = indices[:n_train]\n",
    "        test_idx = indices[n_train:n_train + n_test]\n",
    "        valid_idx = indices[n_train + n_test:]\n",
    "        df.loc[list(train_idx), 'split'] = 'train'\n",
    "        df.loc[list(test_idx), 'split'] = 'test'\n",
    "        df.loc[list(valid_idx), 'split'] = 'valid'\n",
    "        \n",
    "        # Add metadata to text, and truncate...\n",
    "        df['text'] = df['text'].str.slice(0, 500) + ' ... Subject: ' + df['subject'] + ' on ' + df['date']\n",
    "\n",
    "        print(df['subject'].unique())\n",
    "\n",
    "        col = ['text', 'label', 'split']\n",
    "\n",
    "        df_complete = df.loc[:, col] \n",
    "\n",
    "        # Replace labels to text; later code will handle this.\n",
    "        df_complete[\"label\"] = df_complete[\"label\"].replace({0: \"more-suspicious\", 1: \"less-suspicious\"})\n",
    "        \n",
    "\n",
    "\n",
    "        print(df_complete.head(5))\n",
    "\n",
    "        return df_complete\n",
    "\n",
    "    elif which_df == 'preprocessed-pulk17':\n",
    "        # DATA PRE-PROCESSING (MULTI-CLASS DATASET)\n",
    "\n",
    "        # The dataset from Pulk17/Processed-Fake_News-Dataset is actually not properly formatted (the column names are missing)\n",
    "        # Thus, we will read the dataset directly from the url into a pandas DataFrame:\n",
    "\n",
    "        #######################################\n",
    "        #           TRAIN SPLIT\n",
    "        #######################################\n",
    "        # First, read the train.tsv\n",
    "        url = \"https://huggingface.co/datasets/Pulk17/Processed-Fake_News-Dataset/resolve/main/train.tsv\"\n",
    "        df = pd.read_csv(url, sep='\\t', on_bad_lines='skip', engine='python', header=None) # header=None so we can work with default column numbers\n",
    "        print(df.head(5))\n",
    "        df = df.rename(columns={    1 : 'label',\n",
    "                                    2 : 'text'\n",
    "                                })\n",
    "\n",
    "        # Enrich text with available metadata in the other columns\n",
    "        df['text'] = df[\"text\"].astype(str) + ' ' + df.iloc[:, 4:14].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "        df_train = df.iloc[: , 1:3] # grab what seems to be the label and text columns...\n",
    "        df_train[\"split\"] = \"train\"\n",
    "        print(f\"\\n\\ntrain length: {len(df_train)}\\n\\n\", df_train.head(5)) # check the output\n",
    "\n",
    "\n",
    "        #######################################\n",
    "        #           TEST SPLIT\n",
    "        #######################################\n",
    "        # Second, read the test.tsv\n",
    "        url = \"https://huggingface.co/datasets/Pulk17/Processed-Fake_News-Dataset/resolve/main/test.tsv\"\n",
    "        df = pd.read_csv(url, sep='\\t', on_bad_lines='skip', engine='python', header=None)\n",
    "        df = df.rename(columns={    1 : 'label',\n",
    "                                    2 : 'text'\n",
    "                                })\n",
    "        df['text'] = df[\"text\"].astype(str) + ' ' + df.iloc[:, 4:14].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "        df_test = df.iloc[:, 1:3] # grab what seems to be the label and text columns!\n",
    "        df_test[\"split\"] = \"test\"\n",
    "        print(f\"\\n\\ntest length: {len(df_test)}\\n\\n\", df_test.head(10))\n",
    "\n",
    "\n",
    "        #######################################\n",
    "        #           VALIDATION SPLIT\n",
    "        #######################################\n",
    "        # Read the validation split\n",
    "        url = \"https://huggingface.co/datasets/Pulk17/Processed-Fake_News-Dataset/resolve/main/valid.tsv\"\n",
    "        df = pd.read_csv(url, sep='\\t', on_bad_lines='skip', engine='python', header=None)\n",
    "        df = df.rename(columns={    1 : 'label',\n",
    "                                    2 : 'text'\n",
    "                                })\n",
    "        df['text'] = df[\"text\"].astype(str) + ' ' + df.iloc[:, 4:14].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "        df_valid = df.iloc[:, 1:3] # grab what seems to be the label and text columns!\n",
    "        df_valid[\"split\"] = \"valid\"\n",
    "        print(f\"\\n\\nvalidation length: {len(df_valid)}\\n\\n\", df_valid.head(5))\n",
    "\n",
    "\n",
    "\n",
    "        # Just to double-check labels, because I saw one of them said 'pants-fire' lol (I guess as in \"Liar, liar, pants-on-fire\")\n",
    "        print(f\"\\n\\nTraining labels: {df_train['label'].unique()}\")\n",
    "        print(f\"Test labels: {df_test['label'].unique()}\")\n",
    "        print(f\"Validation labels: {df_valid['label'].unique()}\")\n",
    "\n",
    "\n",
    "        # If we need, let's make our dataset from the pandas DataFrame objects... Uncomment if needed\n",
    "        df_complete = pd.concat([df_train, df_test, df_valid])\n",
    "\n",
    "        # remape the multiclass to binary\n",
    "        df_complete['label'] = df_complete[\"label\"].replace({\"true\": \"less-suspicious\",\n",
    "                                                             \"mostly-true\" : \"less-suspicious\",\n",
    "                                                             \"half-true\" : \"less-suspicious\",\n",
    "                                                             \n",
    "                                                             \"false\" : \"more-suspicious\",\n",
    "                                                             \"barely-true\" : \"more-suspicious\",\n",
    "                                                             \"pants-fire\" : \"more-suspicious\"})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print(f\"\\n\\nChecking entries in our df_completed:\\n\\n {df_complete.iloc[[1, 10500, 12500], :]}\")\n",
    "\n",
    "        return df_complete\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "#         Classes and functions for preparing data to train pytorch NN's  \n",
    "##############################################################################\n",
    "class EmbeddingDataset(Dataset):\n",
    "    '''\n",
    "    Description:\n",
    "        Subclass of the Dataset class. Initialized with model name for SentenceTransformer to embed text from a dataset.\n",
    "\n",
    "    Args:\n",
    "        texts := The subsetted text data, ideally from a dataset loaded with load_dataset()\n",
    "        labels := The subsetted label data, again from a dataset loaded with the load_dataset() function call\n",
    "        model_name := a string denoting thje SentenceTransformer() model name. See sentence_transformers module documentation\n",
    "        encode_on_init := boolean flag to specify whether one wants to encode the text upon EmbeddingDataset object initialization\n",
    "        device := specify the device for the model constructor\n",
    "\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, texts, labels, model_name=\"all-MiniLM-L6-v2\", encode_on_init=True, device=\"cpu\", batch_size=64):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "        # mappings for label strings <--> label ID numbers\n",
    "        self.label2id = {label: idx for idx, label in enumerate(sorted(self.labels.unique()))}\n",
    "        self.id2label = {v: k for k, v in self.label2id.items()}\n",
    "\n",
    "        self.label_ids = self.labels.map(self.label2id)\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.model = SentenceTransformer(model_name, device=device)\n",
    "        self.batch_size = batch_size\n",
    "        print(f\"Check for label mapping: {self.label2id}\")\n",
    "\n",
    "\n",
    "        # For embedding all of our text upon initialization...\n",
    "        if encode_on_init:\n",
    "            print(f\"Encoding {len(texts)} texts into embeddings using model {self.model_name}...\")\n",
    "            self.embeddings = self.model.encode(texts, show_progress_bar=True, convert_to_numpy=True, batch_size=self.batch_size)\n",
    "        else:\n",
    "            # Encode lazily per sample, instead of the upfront computation time we incur by embedding everything\n",
    "            self.embeddings = None  \n",
    "\n",
    "    def __len__(self):\n",
    "        # dataset length is just length of our text data\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.embeddings is not None:\n",
    "            # we can return the already-embedded text\n",
    "            x = torch.tensor(self.embeddings[idx], dtype=torch.float32)\n",
    "        else:\n",
    "            # lazy embedding; embed single items as we get them\n",
    "            x = torch.tensor(\n",
    "                self.model.encode([self.texts[idx]], convert_to_numpy=True)[0],\n",
    "                dtype=torch.float32\n",
    "            )\n",
    "        y = torch.tensor(self.label_ids[idx], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def loaders_from_models(df_complete, model_names):\n",
    "    '''\n",
    "    Description:\n",
    "        Takes a list of sentence transformer model names and a complete pandas DataFrame containing train/test/valid splits.\n",
    "        Returns a dictionary of loaders for each split, keyed to the models name\n",
    "    \n",
    "\n",
    "    Args:\n",
    "        model_names := A list of sentence transformer model names, used for accessing those models through the hugging face API to embed our sentences \n",
    "        df_complete := A pandas dataframe with a text and label column, made by concating train/test/valid splits\n",
    "\n",
    "    Output:\n",
    "        Returns a nested dictionary. Keying by model name returns a dictionary of Dataloaders, which are keyed by the data split.\n",
    "        E.g., loader_dict[model_name]['train'] would point to the dataloader for the training split for the given model_name\n",
    "\n",
    "\n",
    "    '''\n",
    "    model_loader_dict = {}\n",
    "\n",
    "    train_split = df_complete[df_complete[\"split\"] == \"train\"].reset_index(drop=True)\n",
    "    test_split = df_complete[df_complete[\"split\"] == \"test\"].reset_index(drop=True)\n",
    "    valid_split = df_complete[df_complete[\"split\"] == \"valid\"].reset_index(drop=True)\n",
    "\n",
    "    for model in model_names:\n",
    "        \n",
    "        loader_dict = {}\n",
    "\n",
    "        print(f\"Creating embedding datasets using embedding model: {model}...\")\n",
    "        train_dataset = EmbeddingDataset(\n",
    "                                            train_split[\"text\"], \n",
    "                                            train_split[\"label\"], \n",
    "                                            model_name=model, \n",
    "                                            device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        loader_dict[\"train\"] = DataLoader(train_dataset,\n",
    "                                            batch_size=32,\n",
    "                                            shuffle=True\n",
    "                                         )\n",
    "        test_dataset = EmbeddingDataset(\n",
    "                                            test_split[\"text\"], \n",
    "                                            test_split[\"label\"], \n",
    "                                            model_name=model, \n",
    "                                            device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        loader_dict[\"test\"] = DataLoader(test_dataset,\n",
    "                                            batch_size=32,\n",
    "                                            shuffle=False\n",
    "                                         )\n",
    "        \n",
    "        valid_dataset = EmbeddingDataset(\n",
    "                                            valid_split[\"text\"], \n",
    "                                            valid_split[\"label\"], \n",
    "                                            model_name=model, \n",
    "                                            device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        loader_dict[\"valid\"] = DataLoader(valid_dataset,\n",
    "                                            batch_size=32,\n",
    "                                            shuffle=False\n",
    "                                         )\n",
    "\n",
    "        model_loader_dict[model] = loader_dict\n",
    "\n",
    "        \n",
    "        \n",
    "    return model_loader_dict, train_dataset, test_dataset, valid_dataset\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "#         Classes and functions for task-specific model training (e.g., for GPT2)  \n",
    "##############################################################################\n",
    "def encode_text(tokenizer, text, return_tensor=False): # referencing Andriy Burkov's \"The Hundred-page Language Models Book\"\n",
    "    \"\"\"\n",
    "    Encodes text using the provided tokenizer, which should come from the pre-trained model somewhere.\n",
    "\n",
    "    Args:\n",
    "        tokenizer: Hugging Face tokenizer\n",
    "        text: Text to encode as tokens\n",
    "        return_tensor: Whether to return PyTorch tensor or not\n",
    "\n",
    "    Returns:\n",
    "        List or tensor of token IDs\n",
    "    \"\"\"\n",
    "    max_length = tokenizer.model_max_length\n",
    "\n",
    "    # If tensor output is requested, encode with PyTorch tensors\n",
    "    if return_tensor:\n",
    "        return tokenizer.encode(\n",
    "            text, \n",
    "            add_special_tokens=False,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=max_length\n",
    "        )\n",
    "    # Otherwise return list of token IDs\n",
    "    else:\n",
    "        return tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "class TextClassificationDataset(Dataset): # referencing Andriy Burkov's \"The Hundred-page Language Models Book\"\n",
    "    \"\"\"\n",
    "    Dataset class for text classification using a pre-trained model.\n",
    "    Converts text and labels into model-ready format.\n",
    "    Follows same concept as the EmbeddingDataset class and creates its own label2id mapping.\n",
    "\n",
    "    Args:\n",
    "        data: pandas DataFrame object with text and label\n",
    "        tokenizer: Hugging Face tokenizer from a pre-trained model\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, data, tokenizer):\n",
    "        print(\"Initializing TextClassificationDataset...\")\n",
    "        self.data = data # the full dataframe\n",
    "        self.text = self.data['text']\n",
    "        self.labels = self.data['label']\n",
    "        self.tokenizer = tokenizer\n",
    "        self.unique_labels = sorted(self.labels.unique())\n",
    "        print(f\"Checking label consistency: {self.unique_labels}\")\n",
    "\n",
    "        # Creates its own mapping\n",
    "        self.label2id = {label: idx for idx, label in enumerate(sorted(self.labels.unique()))}\n",
    "        self.id2label = {v: k for k, v in self.label2id.items()}\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return total number of examples\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get example(single row or record) from dataset; this is a lazy encoding\n",
    "        item = self.data.iloc[idx]\n",
    "        # Convert text to token IDs\n",
    "        input_ids = encode_text(self.tokenizer, item[\"text\"])\n",
    "        # Convert label string to ID\n",
    "        labels = self.label2id[item[\"label\"]]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels\n",
    "        }   \n",
    "\n",
    "def collate_fn(batch): # referencing Andriy Burkov's \"The Hundred-page Language Models Book\"\n",
    "    \"\"\"\n",
    "    Collates batch of examples into training-ready format.\n",
    "    Handles padding and conversion to tensors.\n",
    "\n",
    "    Args:\n",
    "        batch: List of examples from Dataset\n",
    "\n",
    "    Returns:\n",
    "        dict: Contains input_ids, labels, and attention_mask tensors\n",
    "    \"\"\"\n",
    "    # Find longest sequence for \n",
    "    # \n",
    "    # ; recall from our lecture about padding up to batch's max length\n",
    "    max_length = max(len(item[\"input_ids\"]) for item in batch)\n",
    "\n",
    "    # Pad input sequences with zeros;\n",
    "    input_ids = [\n",
    "        item[\"input_ids\"] +\n",
    "        [0] * (max_length - len(item[\"input_ids\"]))\n",
    "        for item in batch\n",
    "    ]\n",
    "\n",
    "    # Create attention masks (1 for tokens, 0 for padding); we do this just by concat\n",
    "    attention_mask = [\n",
    "        [1] * len(item[\"input_ids\"]) +\n",
    "        [0] * (max_length - len(item[\"input_ids\"]))\n",
    "        for item in batch\n",
    "    ]\n",
    "\n",
    "    # Collect labels\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "\n",
    "    # Convert everything to tensors\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(input_ids),\n",
    "        \"labels\": torch.tensor(labels),\n",
    "        \"attention_mask\": torch.tensor(attention_mask)\n",
    "    }\n",
    "\n",
    "def prepare_data(df_complete, tokenizer, batch_size, shuffle=True): # referencing Andriy Burkov's \"The Hundred-page Language Models Book\"\n",
    "\n",
    "    '''\n",
    "    Takes a DataFrame containing three splits of data: train, test and valid.\n",
    "    Prepares dataloaders for those splits.\n",
    "    '''\n",
    "\n",
    "    # Grab the splits! Just as in the loaders_from_models()\n",
    "    train_split = df_complete[df_complete[\"split\"] == \"train\"]\n",
    "    test_split = df_complete[df_complete[\"split\"] == \"test\"]\n",
    "    valid_split = df_complete[df_complete[\"split\"] == \"valid\"]\n",
    "\n",
    "    # Make the classification datasets, and pull whatever attributes you need from the object\n",
    "    train_dataset = TextClassificationDataset(\n",
    "        train_split,\n",
    "        tokenizer\n",
    "    )\n",
    "    print(f\"Checking train label maps. label2id: {train_dataset.label2id}, id2label: {train_dataset.id2label}\")\n",
    "    label2id = train_dataset.label2id\n",
    "    id2label = train_dataset.id2label\n",
    "    unique_labels = train_dataset.unique_labels\n",
    "\n",
    "    # NOTE: We could probably combine the test and valid dataset, to be honest! May not need valid split for this.\n",
    "    test_dataset = TextClassificationDataset(\n",
    "        test_split,\n",
    "        tokenizer\n",
    "    )\n",
    "    print(f\"Checking test label maps. label2id: {test_dataset.label2id}, id2label: {test_dataset.id2label}\")\n",
    "          \n",
    "    valid_dataset = TextClassificationDataset(\n",
    "        valid_split,\n",
    "        tokenizer\n",
    "    )\n",
    "    print(f\"Checking valid label maps. label2id: {valid_dataset.label2id}, id2label: {valid_dataset.id2label}\")\n",
    "\n",
    "\n",
    "    # Make the dataloaders!\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    valid_dataloader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "\n",
    "    return train_dataloader, test_dataloader, valid_dataloader, label2id, id2label, unique_labels, train_dataset, test_dataset, valid_dataset\n",
    "\n",
    "def compute_metrics(pred): # referencing the \"Natural Language Processing with Transformers\" book (pg. 47); May not need\n",
    "    # CODE HERE\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    #recall = recall_score(labels, pred)\n",
    "    #precision = precision_score(labels, pred)\n",
    "    return {\"accuracy\" : acc, \"f1\" : f1}\n",
    "\n",
    "def get_batch_embeddings(batch):\n",
    "    '''\n",
    "        Embeds a batch of text data. This code takes the LAST hidden state.\n",
    "        The last hidden state will have embeddings for each token, so we do mean-pooling to get a whole \"sentence\" embedding.\n",
    "        This is primarily for getting embeddings using our fine-tuned model, so that we can visualize it with a technique like UMAP.\n",
    "    '''\n",
    "    inputs = tokenizer(batch[\"text\"], return_tensors=\"pt\",\n",
    "                       padding=True, truncation=True, max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.base_model(**inputs, output_hidden_states=True)\n",
    "        last_hidden = outputs.hidden_states[-1]\n",
    "        mean_embeddings = last_hidden.mean(dim=1).cpu().numpy()\n",
    "    return {\"embedding\": mean_embeddings}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b8c1bb",
   "metadata": {},
   "source": [
    "## Get data and check it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0eef87b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                              title  \\\n",
      "0        2619  Ex-CIA head says Trump remarks on Russia inter...   \n",
      "1       16043  YOU WON’T BELIEVE HIS PUNISHMENT! HISPANIC STO...   \n",
      "2         876  Federal Reserve governor Powell's policy views...   \n",
      "3       19963  SCOUNDREL HILLARY SUPPORTER STARTS “TrumpLeaks...   \n",
      "4       10783  NANCY PELOSI ARROGANTLY DISMISSES Questions on...   \n",
      "\n",
      "                                                text          subject  \\\n",
      "0  Former CIA director John Brennan on Friday cri...     politicsNews   \n",
      "1  How did this man come to OWN this store? There...  Government News   \n",
      "2  President Donald Trump on Thursday tapped Fede...     politicsNews   \n",
      "3  Hillary Clinton ally David Brock is offering t...        left-news   \n",
      "4  Pleading ignorance is a perfect ploy for Nancy...         politics   \n",
      "\n",
      "                date  label  \n",
      "0     July 22, 2017       1  \n",
      "1       Jun 19, 2017      0  \n",
      "2  November 2, 2017       1  \n",
      "3       Sep 17, 2016      0  \n",
      "4       May 26, 2017      0  \n",
      "Former CIA director John Brennan on Friday criticized as “disgraceful” President Donald Trump’s efforts to play down U.S. intelligence agencies’ assessment that Russia meddled in the 2016 U.S. election. Trump’s administration has been dogged by investigations into allegations of Russian interference in last year’s U.S. presidential election and possible ties with his campaign team. Speaking one day before his first meeting with Russian President Vladimir Putin in Hamburg earlier this month, Trump said he suspected Russian interference in the election but that no one knows for sure. “These types of comments are just disgraceful ... and the person who said them should be ashamed of himself,” said Brennan, CIA chief under former President Barack Obama, at the Aspen Security Forum. Special Counsel Robert Mueller and several U.S. congressional committees are investigating whether Russia interfered in the election and colluded with Trump’s campaign to try to swing the race in his favor over Democratic rival Hillary Clinton.  Those probes are focused almost exclusively on Moscow’s actions, lawmakers and intelligence officials have said, and no evidence has surfaced publicly implicating other countries. Moscow has denied any interference, and Trump has said that  his campaign did not collude with Russia. Brennan said he was disappointed by the president’s handling of security issues in his first six months in office. “I must say there are disappointments that I see in terms of what Mr. Trump is doing on the international stage that I think pose serious questions about how he is keeping safe our national security,” Brennan said. Speaking at the same event in Aspen, James Clapper, the former director of national intelligence under Obama, was also critical of Trump’s administration. Asked if Jared Kushner, Trump’s son-in-law and top adviser, should have his security clearance canceled for initially failing to list on a disclosure form contacts he had with Russians, Clapper said it should be suspended pending a review. “I do think the appropriate thing here is take a pause and at least suspend a clearance until you’ve had the opportunity to investigate and then decide whether the clearance should be restored or not,” Clapper told the same panel. Brennan and Clapper also criticized Trump’s remarks in a tweet earlier this year about U.S. spy agencies in which he accused them of practices reminiscent of Nazi Germany.  “That (tweet) was a terrible insulting affront not to me or John. We get paid the big bucks to take that. But I’m talking about the rank and file, the people in the trenches, men and women, the patriots in the intelligence community and that was completely inappropriate,” said Clapper.\n",
      "['politicsNews' 'Government News' 'left-news' 'politics' 'worldnews'\n",
      " 'News' 'Middle-east' 'US_News']\n",
      "                                                text            label  split\n",
      "0  Former CIA director John Brennan on Friday cri...  less-suspicious  train\n",
      "1  How did this man come to OWN this store? There...  more-suspicious  train\n",
      "2  President Donald Trump on Thursday tapped Fede...  less-suspicious  train\n",
      "3  Hillary Clinton ally David Brock is offering t...  more-suspicious  train\n",
      "4  Pleading ignorance is a perfect ploy for Nancy...  more-suspicious  train\n",
      "\n",
      "\n",
      "Max length text: 647\n",
      "split\n",
      "train    24000\n",
      "valid     3000\n",
      "test      3000\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "more-suspicious    15478\n",
      "less-suspicious    14522\n",
      "Name: count, dtype: int64\n",
      "Former CIA director John Brennan on Friday criticized as “disgraceful” President Donald Trump’s efforts to play down U.S. intelligence agencies’ assessment that Russia meddled in the 2016 U.S. election. Trump’s administration has been dogged by investigations into allegations of Russian interference in last year’s U.S. presidential election and possible ties with his campaign team. Speaking one day before his first meeting with Russian President Vladimir Putin in Hamburg earlier this month, Trum ... Subject: politicsNews on July 22, 2017 \n",
      "less-suspicious\n",
      "How did this man come to OWN this store? There is no information on much about this fraudster except that he stole from Americans and is getting just a slap on the wrist. It s no wonder these people are repeat offenders!A hispanic store owner testified in court through a Spanish translator (Why no English?) that he swindled the American tax payers out of millions via food stamp fraud. Eduardo Leonardo plead guilty to stealing from the USDA by illegally exchanging over a million dollars of food s ... Subject: Government News on Jun 19, 2017\n",
      "more-suspicious\n"
     ]
    }
   ],
   "source": [
    "# Get our complete dataframe, which should have all three splits (pandas DataFrame)\n",
    "# as a note: pulk17 is the binary labels, preprocessed-pulk17 has the multi-class labels\n",
    "which_df = 'pulk17'\n",
    "\n",
    "df_complete = get_df(which_df)\n",
    "\n",
    "# Check data details\n",
    "maxlen = 0\n",
    "for text in df_complete['text']:\n",
    "    if len(text) > maxlen:\n",
    "        maxlen = len(text)\n",
    "print(f\"\\n\\nMax length text: {maxlen}\")\n",
    "\n",
    "# quick check on our split sizes\n",
    "print(df_complete['split'].value_counts())\n",
    "\n",
    "# quick check on our label counts\n",
    "print(df_complete['label'].value_counts())\n",
    "\n",
    "print(df_complete['text'][0])\n",
    "print(df_complete['label'][0])\n",
    "\n",
    "print(df_complete['text'][1])\n",
    "print(df_complete['label'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47421c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embedding datasets using embedding model: all-mpnet-base-v2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "397695c2c68f466dad8268a08ac194a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\preet\\anaconda3\\envs\\nlp590\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\preet\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a804f15b3c04416b868cd864ff583a61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11163e9db1ca48d4a02baafe0efd90d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db10d30661c0477cb90b8c9e2773ad89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "060f66c71f4e43509ec7299bc65a1353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa59a9d823574df0a28d9d63d8b819ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/sentence-transformers/all-mpnet-base-v2/resolve/main/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf75c05857db44d09b7331464715c171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  57%|#####7    | 252M/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e503db77e4c4a4495722e4f8276e428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09d4dd2d82bd4390a908b491f53ad1de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7edfbf33124a47848cfdfcf84fe01f7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78ee2ef0afa74554a4a7a8d33a8e8115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15404d4684d649a5a420c4d0847a8dea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for label mapping: {'less-suspicious': 0, 'more-suspicious': 1}\n",
      "Encoding 24000 texts into embeddings using model all-mpnet-base-v2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc1d9ec72374e8bab3d57d999f7d7e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      6\u001b[39m embedding_models = [\u001b[33m'\u001b[39m\u001b[33mall-mpnet-base-v2\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      7\u001b[39m                     \u001b[33m'\u001b[39m\u001b[33mall-distilroberta-v1\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      8\u001b[39m                     \u001b[33m'\u001b[39m\u001b[33mall-MiniLM-L12-v2\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      9\u001b[39m                     \u001b[33m'\u001b[39m\u001b[33mall-MiniLM-L6-v2\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# For each model, make a set of dataloaders for train, test and split!\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m loader_dict, train_dataset, test_dataset, valid_dataset = loaders_from_models(df_complete, embedding_models)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Test the batching for our data:\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m embedding_models:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 261\u001b[39m, in \u001b[36mloaders_from_models\u001b[39m\u001b[34m(df_complete, model_names)\u001b[39m\n\u001b[32m    258\u001b[39m loader_dict = {}\n\u001b[32m    260\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCreating embedding datasets using embedding model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m train_dataset = EmbeddingDataset(\n\u001b[32m    262\u001b[39m                                     train_split[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m], \n\u001b[32m    263\u001b[39m                                     train_split[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m], \n\u001b[32m    264\u001b[39m                                     model_name=model, \n\u001b[32m    265\u001b[39m                                     device=\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    267\u001b[39m loader_dict[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m] = DataLoader(train_dataset,\n\u001b[32m    268\u001b[39m                                     batch_size=\u001b[32m32\u001b[39m,\n\u001b[32m    269\u001b[39m                                     shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    270\u001b[39m                                  )\n\u001b[32m    271\u001b[39m test_dataset = EmbeddingDataset(\n\u001b[32m    272\u001b[39m                                     test_split[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m], \n\u001b[32m    273\u001b[39m                                     test_split[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m], \n\u001b[32m    274\u001b[39m                                     model_name=model, \n\u001b[32m    275\u001b[39m                                     device=\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 210\u001b[39m, in \u001b[36mEmbeddingDataset.__init__\u001b[39m\u001b[34m(self, texts, labels, model_name, encode_on_init, device, batch_size)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m encode_on_init:\n\u001b[32m    209\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEncoding \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(texts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m texts into embeddings using model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m     \u001b[38;5;28mself\u001b[39m.embeddings = \u001b[38;5;28mself\u001b[39m.model.encode(texts, show_progress_bar=\u001b[38;5;28;01mTrue\u001b[39;00m, convert_to_numpy=\u001b[38;5;28;01mTrue\u001b[39;00m, batch_size=\u001b[38;5;28mself\u001b[39m.batch_size)\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    212\u001b[39m     \u001b[38;5;66;03m# Encode lazily per sample, instead of the upfront computation time we incur by embedding everything\u001b[39;00m\n\u001b[32m    213\u001b[39m     \u001b[38;5;28mself\u001b[39m.embeddings = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\anaconda3\\envs\\nlp590\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\anaconda3\\envs\\nlp590\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1094\u001b[39m, in \u001b[36mSentenceTransformer.encode\u001b[39m\u001b[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001b[39m\n\u001b[32m   1091\u001b[39m features.update(extra_features)\n\u001b[32m   1093\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m-> \u001b[39m\u001b[32m1094\u001b[39m     out_features = \u001b[38;5;28mself\u001b[39m.forward(features, **kwargs)\n\u001b[32m   1095\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device.type == \u001b[33m\"\u001b[39m\u001b[33mhpu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1096\u001b[39m         out_features = copy.deepcopy(out_features)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\anaconda3\\envs\\nlp590\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1175\u001b[39m, in \u001b[36mSentenceTransformer.forward\u001b[39m\u001b[34m(self, input, **kwargs)\u001b[39m\n\u001b[32m   1169\u001b[39m             module_kwarg_keys = \u001b[38;5;28mself\u001b[39m.module_kwargs.get(module_name, [])\n\u001b[32m   1170\u001b[39m         module_kwargs = {\n\u001b[32m   1171\u001b[39m             key: value\n\u001b[32m   1172\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs.items()\n\u001b[32m   1173\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(module, \u001b[33m\"\u001b[39m\u001b[33mforward_kwargs\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module.forward_kwargs)\n\u001b[32m   1174\u001b[39m         }\n\u001b[32m-> \u001b[39m\u001b[32m1175\u001b[39m     \u001b[38;5;28minput\u001b[39m = module(\u001b[38;5;28minput\u001b[39m, **module_kwargs)\n\u001b[32m   1176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\anaconda3\\envs\\nlp590\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\anaconda3\\envs\\nlp590\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\anaconda3\\envs\\nlp590\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:261\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, features, **kwargs)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    239\u001b[39m \u001b[33;03mForward pass through the transformer model.\u001b[39;00m\n\u001b[32m    240\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    257\u001b[39m \u001b[33;03m        - 'all_layer_embeddings': If the model outputs hidden states, contains embeddings from all layers\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    259\u001b[39m trans_features = {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features.items() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_forward_params}\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m.auto_model(**trans_features, **kwargs, return_dict=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    262\u001b[39m token_embeddings = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    263\u001b[39m features[\u001b[33m\"\u001b[39m\u001b[33mtoken_embeddings\u001b[39m\u001b[33m\"\u001b[39m] = token_embeddings\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\anaconda3\\envs\\nlp590\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\anaconda3\\envs\\nlp590\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\anaconda3\\envs\\nlp590\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:486\u001b[39m, in \u001b[36mMPNetModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[39m\n\u001b[32m    484\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m    485\u001b[39m embedding_output = \u001b[38;5;28mself\u001b[39m.embeddings(input_ids=input_ids, position_ids=position_ids, inputs_embeds=inputs_embeds)\n\u001b[32m--> \u001b[39m\u001b[32m486\u001b[39m encoder_outputs = \u001b[38;5;28mself\u001b[39m.encoder(\n\u001b[32m    487\u001b[39m     embedding_output,\n\u001b[32m    488\u001b[39m     attention_mask=extended_attention_mask,\n\u001b[32m    489\u001b[39m     head_mask=head_mask,\n\u001b[32m    490\u001b[39m     output_attentions=output_attentions,\n\u001b[32m    491\u001b[39m     output_hidden_states=output_hidden_states,\n\u001b[32m    492\u001b[39m     return_dict=return_dict,\n\u001b[32m    493\u001b[39m )\n\u001b[32m    494\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    495\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\anaconda3\\envs\\nlp590\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\anaconda3\\envs\\nlp590\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\anaconda3\\envs\\nlp590\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:338\u001b[39m, in \u001b[36mMPNetEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[39m\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    336\u001b[39m     all_hidden_states = all_hidden_states + (hidden_states,)\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m layer_outputs = layer_module(\n\u001b[32m    339\u001b[39m     hidden_states,\n\u001b[32m    340\u001b[39m     attention_mask,\n\u001b[32m    341\u001b[39m     head_mask[i],\n\u001b[32m    342\u001b[39m     position_bias,\n\u001b[32m    343\u001b[39m     output_attentions=output_attentions,\n\u001b[32m    344\u001b[39m     **kwargs,\n\u001b[32m    345\u001b[39m )\n\u001b[32m    346\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\anaconda3\\envs\\nlp590\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\anaconda3\\envs\\nlp590\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\anaconda3\\envs\\nlp590\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:297\u001b[39m, in \u001b[36mMPNetLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, position_bias, output_attentions, **kwargs)\u001b[39m\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    289\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    290\u001b[39m     hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    295\u001b[39m     **kwargs,\n\u001b[32m    296\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m     self_attention_outputs = \u001b[38;5;28mself\u001b[39m.attention(\n\u001b[32m    298\u001b[39m         hidden_states,\n\u001b[32m    299\u001b[39m         attention_mask,\n\u001b[32m    300\u001b[39m         head_mask,\n\u001b[32m    301\u001b[39m         position_bias=position_bias,\n\u001b[32m    302\u001b[39m         output_attentions=output_attentions,\n\u001b[32m    303\u001b[39m     )\n\u001b[32m    304\u001b[39m     attention_output = self_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    305\u001b[39m     outputs = self_attention_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add self attentions if we output attention weights\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\anaconda3\\envs\\nlp590\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\anaconda3\\envs\\nlp590\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\anaconda3\\envs\\nlp590\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:238\u001b[39m, in \u001b[36mMPNetAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, position_bias, output_attentions, **kwargs)\u001b[39m\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    230\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    231\u001b[39m     hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    236\u001b[39m     **kwargs,\n\u001b[32m    237\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m     self_outputs = \u001b[38;5;28mself\u001b[39m.attn(\n\u001b[32m    239\u001b[39m         hidden_states,\n\u001b[32m    240\u001b[39m         attention_mask,\n\u001b[32m    241\u001b[39m         head_mask,\n\u001b[32m    242\u001b[39m         position_bias,\n\u001b[32m    243\u001b[39m         output_attentions=output_attentions,\n\u001b[32m    244\u001b[39m     )\n\u001b[32m    245\u001b[39m     attention_output = \u001b[38;5;28mself\u001b[39m.LayerNorm(\u001b[38;5;28mself\u001b[39m.dropout(self_outputs[\u001b[32m0\u001b[39m]) + hidden_states)\n\u001b[32m    246\u001b[39m     outputs = (attention_output,) + self_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\anaconda3\\envs\\nlp590\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\anaconda3\\envs\\nlp590\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\anaconda3\\envs\\nlp590\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:158\u001b[39m, in \u001b[36mMPNetSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, position_bias, output_attentions, **kwargs)\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    148\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    149\u001b[39m     hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    154\u001b[39m     **kwargs,\n\u001b[32m    155\u001b[39m ):\n\u001b[32m    156\u001b[39m     batch_size, seq_length, _ = hidden_states.shape\n\u001b[32m    157\u001b[39m     q = (\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m         \u001b[38;5;28mself\u001b[39m.q(hidden_states)\n\u001b[32m    159\u001b[39m         .view(batch_size, -\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.num_attention_heads, \u001b[38;5;28mself\u001b[39m.attention_head_size)\n\u001b[32m    160\u001b[39m         .transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    161\u001b[39m     )\n\u001b[32m    162\u001b[39m     k = (\n\u001b[32m    163\u001b[39m         \u001b[38;5;28mself\u001b[39m.k(hidden_states)\n\u001b[32m    164\u001b[39m         .view(batch_size, -\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.num_attention_heads, \u001b[38;5;28mself\u001b[39m.attention_head_size)\n\u001b[32m    165\u001b[39m         .transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    166\u001b[39m     )\n\u001b[32m    167\u001b[39m     v = (\n\u001b[32m    168\u001b[39m         \u001b[38;5;28mself\u001b[39m.v(hidden_states)\n\u001b[32m    169\u001b[39m         .view(batch_size, -\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.num_attention_heads, \u001b[38;5;28mself\u001b[39m.attention_head_size)\n\u001b[32m    170\u001b[39m         .transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    171\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\anaconda3\\envs\\nlp590\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\anaconda3\\envs\\nlp590\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\anaconda3\\envs\\nlp590\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.linear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m.weight, \u001b[38;5;28mself\u001b[39m.bias)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "#           MAKE DATA LOADERS (FOR TRAINING NN)\n",
    "#######################################\n",
    "\n",
    "# Define our list of models:\n",
    "embedding_models = ['all-mpnet-base-v2',\n",
    "                    'all-distilroberta-v1',\n",
    "                    'all-MiniLM-L12-v2',\n",
    "                    'all-MiniLM-L6-v2']\n",
    "\n",
    "# For each model, make a set of dataloaders for train, test and split!\n",
    "loader_dict, train_dataset, test_dataset, valid_dataset = loaders_from_models(df_complete, embedding_models)\n",
    "\n",
    "# Test the batching for our data:\n",
    "for model in embedding_models:\n",
    "    print(f\"Checking batches from loaders for model {model}\")\n",
    "    for xb, yb in loader_dict[model]['train']:\n",
    "        print(\"\\tTrain batch:\", xb.shape, yb.shape)\n",
    "        break\n",
    "    for xb, yb in loader_dict[model]['test']:\n",
    "        print(\"\\tTest batch:\", xb.shape, yb.shape)\n",
    "        break \n",
    "    for xb, yb in loader_dict[model]['valid']:\n",
    "        print(\"\\tValid batch:\", xb.shape, yb.shape)\n",
    "        break\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7abed89",
   "metadata": {},
   "source": [
    "## Simple code for training with the embedded datasets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd66468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE FOR TORCH NN\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e918e31",
   "metadata": {},
   "source": [
    "# GPT-2 for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "08409310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22376d170a444018abe24b4dccf83d9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e76c6630a642cea2480a126ba2c13c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c900dad533b42eb8c2ded49a845e6c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62bbab1ba8d64f0da0b92ba0b49bbc21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1257fb70ce0b42fea4d6885d6440b187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "481b067001cf43d1bfd6e45fea24ccda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "843765b7fd9a4634b559c32572367cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing TextClassificationDataset...\n",
      "Checking label consistency: ['less-suspicious', 'more-suspicious']\n",
      "Checking train label maps. label2id: {'less-suspicious': 0, 'more-suspicious': 1}, id2label: {0: 'less-suspicious', 1: 'more-suspicious'}\n",
      "Initializing TextClassificationDataset...\n",
      "Checking label consistency: ['less-suspicious', 'more-suspicious']\n",
      "Checking test label maps. label2id: {'less-suspicious': 0, 'more-suspicious': 1}, id2label: {0: 'less-suspicious', 1: 'more-suspicious'}\n",
      "Initializing TextClassificationDataset...\n",
      "Checking label consistency: ['less-suspicious', 'more-suspicious']\n",
      "Checking valid label maps. label2id: {'less-suspicious': 0, 'more-suspicious': 1}, id2label: {0: 'less-suspicious', 1: 'more-suspicious'}\n",
      "['less-suspicious', 'more-suspicious']\n",
      "['less-suspicious', 'more-suspicious']\n",
      "['less-suspicious', 'more-suspicious']\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "# Prepare data, set up tokenizer\n",
    "##########################\n",
    "\n",
    "# Set hyperparameters\n",
    "num_epochs, batch_size, learning_rate = set_hyperparameters()\n",
    "\n",
    "# Set Hugging Face model name\n",
    "model_name = \"openai-community/gpt2\"\n",
    "\n",
    "# Initialize and configure tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, \n",
    "                                          local_files_only=False, \n",
    "                                          force_download=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Prepares the dataset; we likely won't need the dataloaders if we use the Trainer, but here they are anyways! \n",
    "_, _, _, label2id, id2label, unique_labels, train_dataset, test_dataset, valid_dataset = prepare_data(\n",
    "    df_complete,\n",
    "    tokenizer,\n",
    "    batch_size\n",
    ")\n",
    "\n",
    "##########################\n",
    "# Load pretrained model, set up trainer\n",
    "##########################\n",
    "\n",
    "# Initialize the model and the tokenizer...\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(unique_labels)\n",
    ")\n",
    "\n",
    "# setup device...\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Configure padding and label mappings\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id\n",
    "\n",
    "# Check label order for consistency!\n",
    "print(train_dataset.unique_labels)\n",
    "print(test_dataset.unique_labels)\n",
    "print(valid_dataset.unique_labels)\n",
    "\n",
    "# Source - https://stackoverflow.com/questions/62641972/hugging-face-transformers-loading-model-from-path-error\n",
    "# Posted by cronoik\n",
    "# Retrieved 11/4/2025, License - CC-BY-SA 4.0\n",
    "\n",
    "# For saving the model repo files locally, if we want to access it offline.\n",
    "# tokenizer.save_pretrained('gpt2')\n",
    "# config = AutoConfig.from_pretrained(model_name)\n",
    "# config.save_pretrained('gpt2')\n",
    "\n",
    "\n",
    "######################################################\n",
    "# using the transformers.Trainer (referencing the Hugging Face book from class)\n",
    "######################################################\n",
    "\n",
    "# This is another way to move forwaard, but from what I understand, gpt2 does not include any padding in its tokenizer...\n",
    "# So either we move to another model or implement that.\n",
    "# If you ran the earlier code, the padding tokens are set. And for the Trainer, we pass the collate_fn as our data_collator...\n",
    "\n",
    "logging_steps = len(train_dataset) // batch_size\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"training-args-\" + model_name,\n",
    "                                  eval_strategy=\"epoch\",\n",
    "                                  num_train_epochs=num_epochs,\n",
    "                                  learning_rate=learning_rate,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  weight_decay=0.01,\n",
    "                                  disable_tqdm=False,\n",
    "                                  eval_steps=logging_steps,\n",
    "                                  logging_steps =logging_steps,\n",
    "                                  push_to_hub=False,\n",
    "                                  log_level=\"error\")\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  args=training_args,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  train_dataset=train_dataset,\n",
    "                  eval_dataset=test_dataset,\n",
    "                  data_collator=collate_fn\n",
    "                  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22ab0c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 3:32:17, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.032700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=750, training_loss=0.03273438262939453, metrics={'train_runtime': 12739.1529, 'train_samples_per_second': 1.884, 'train_steps_per_second': 0.059, 'total_flos': 1656196791533568.0, 'train_loss': 0.03273438262939453, 'epoch': 1.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start trainer\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e5dbed51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.4066664277834207e-08,\n",
       " 'eval_accuracy': 1.0,\n",
       " 'eval_f1': 1.0,\n",
       " 'eval_runtime': 528.3926,\n",
       " 'eval_samples_per_second': 5.678,\n",
       " 'eval_steps_per_second': 0.178,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate with the test set\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9505bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 4.220064056426054e-06,\n",
       " 'eval_accuracy': 1.0,\n",
       " 'eval_f1': 1.0,\n",
       " 'eval_runtime': 628.8119,\n",
       " 'eval_samples_per_second': 4.771,\n",
       " 'eval_steps_per_second': 0.149,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also evaluate with the other validation dataset\n",
    "trainer.evaluate(eval_dataset=valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8a332d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('training-args-openai-community/after-train-11-8-2025-v2\\\\tokenizer_config.json',\n",
       " 'training-args-openai-community/after-train-11-8-2025-v2\\\\special_tokens_map.json',\n",
       " 'training-args-openai-community/after-train-11-8-2025-v2\\\\vocab.json',\n",
       " 'training-args-openai-community/after-train-11-8-2025-v2\\\\merges.txt',\n",
       " 'training-args-openai-community/after-train-11-8-2025-v2\\\\added_tokens.json',\n",
       " 'training-args-openai-community/after-train-11-8-2025-v2\\\\tokenizer.json')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saves model to local\n",
    "# the model dated to 11-8-2025-v2 will be the latest one...\n",
    "# v3 will be anything after, if needed...\n",
    "\n",
    "# UNCOMMENT IF NEEDED\n",
    "# model_save_path = \"training-args-openai-community/after-train-11-8-2025-v3\"\n",
    "# trainer.save_model(model_save_path)\n",
    "# tokenizer.save_pretrained(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd54a718",
   "metadata": {},
   "source": [
    "## Load the fine-tuned model and eval on a different dataset\n",
    "We fine-tuned the model with 3 epochs on the Pulk17 dataset (N=30,000 with binary class).\n",
    "\n",
    "Now, we prepare the Pre-processed Pulk17 dataset (N=12,745) in a similar way.\n",
    "\n",
    "This preprocessed dataset seemed a bit problematic, and I suspect the quality of the labels aren't as good. But let's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e00e628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the path to which we saved the trained model\n",
    "model_path = \"training-args-openai-community/after-train-11-8-2025-v2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "# get our batch_size...\n",
    "_, batch_size, _ = set_hyperparameters()\n",
    "\n",
    "# load tokenizer from the local model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "######    Get data    #######\n",
    "which_df = 'preprocessed-pulk17'\n",
    "df_complete = get_df(which_df)\n",
    "#############################\n",
    "\n",
    "# Here, we can test with the other dataset! So let's prepare it.\n",
    "_, _, _, label2id, id2label, unique_labels, train_dataset, test_dataset, valid_dataset = prepare_data(\n",
    "    df_complete,\n",
    "    tokenizer,\n",
    "    batch_size\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  data_collator=collate_fn,\n",
    "                  compute_metrics=compute_metrics)\n",
    "\n",
    "# Really, this is an entirely new dataset (preprocessed-pulk17 with a model fine-tuned with the pulk17 dataset)\n",
    "# So we can evaluate it on the entire dataset for fun.\n",
    "trainer.evaluate(eval_dataset = test_dataset)\n",
    "\n",
    "trainer.evaluate(eval_dataset = valid_dataset)\n",
    "\n",
    "trainer.evaluate(eval_dataset = test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d2fd9a",
   "metadata": {},
   "source": [
    "# Demo: Test your own text for fun\n",
    "*I generated an example suspicious news story by prompting ChatGPT on 11-8-2025:*\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "Give me an example of a suspicious news story. It should be at least 500 characters.\n",
    "\n",
    "At the end, append \"... Subject: <subject> on <Month> <day> <year>\" where <subject> can be something in the list ['politicsNews' 'Government News' 'left-news' 'politics' 'worldnews' 'News' 'Middle-east' 'US_News'], and the date is around 2016 or 2017...```\n",
    "\n",
    "```\n",
    "\n",
    "The output was the following:\n",
    "\n",
    "```\n",
    "A secret document allegedly leaked from an unnamed government source claims that several U.S. senators held private meetings with foreign energy executives to negotiate control over future oil imports.\n",
    "The report, circulated on obscure blogs and social media, cites “anonymous insiders” but offers no verifiable evidence or supporting documents.\n",
    "Despite multiple denials from officials, the story continues to gain traction online, fueling public mistrust. .. Subject: politicsNews on March 14 2017\n",
    "```\n",
    "\n",
    "This was followed by: \"Nice. Can you make it a little more sensational?\"\n",
    "\n",
    "This produced:\n",
    "\n",
    "```\n",
    "Example sensational suspicious story:\n",
    "A shocking leak late last night claims that top U.S. lawmakers secretly met aboard a private jet to strike hidden energy deals with foreign powers. Sources say the hush-hush gathering was “arranged in total darkness,” with participants leaving phones behind to avoid tracking. Eyewitnesses describe convoys arriving at a secluded airstrip outside D.C. While no official statements have been made, online forums are exploding with claims of a massive political cover-up.\n",
    "\n",
    "... Subject: Government News on February 3 2016\n",
    "```\n",
    "\n",
    "And I added some more flamboyant language to get the following:\n",
    "\n",
    "```\n",
    "A shocking leak late last night claims that top U.S. lawmakers secretly met aboard a private jet to strike hidden energy deals with foreign powers.\n",
    "How can they DO such a thing? Do they think the American people are stupid?\n",
    "Sources say the hush-hush gathering was “arranged in total darkness,” with participants leaving phones behind to avoid tracking.\n",
    "Eyewitnesses describe convoys arriving at a secluded airstrip outside D.C.\n",
    "While no official statements have been made, online forums are exploding with claims of a massive political cover-up ... Subject: Government News on February 3 2016\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf73078d",
   "metadata": {},
   "source": [
    "### Demo code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b878090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_sus\n",
      "Probabilities:  tensor([[0.0425, 0.9575]], device='cuda:0')\n",
      "Predicted class:  1\n",
      "Predicted label:  more-suspicious\n",
      "\n",
      "\n",
      "\n",
      "less_sus\n",
      "Probabilities:  tensor([[1.0000e+00, 5.7998e-07]], device='cuda:0')\n",
      "Predicted class:  0\n",
      "Predicted label:  less-suspicious\n",
      "\n",
      "\n",
      "\n",
      "more_sus\n",
      "Probabilities:  tensor([[7.5841e-10, 1.0000e+00]], device='cuda:0')\n",
      "Predicted class:  1\n",
      "Predicted label:  more-suspicious\n",
      "\n",
      "\n",
      "\n",
      "facebook_news\n",
      "Probabilities:  tensor([[1.6502e-04, 9.9984e-01]], device='cuda:0')\n",
      "Predicted class:  1\n",
      "Predicted label:  more-suspicious\n",
      "\n",
      "\n",
      "\n",
      "kff_news\n",
      "Probabilities:  tensor([[0.7483, 0.2517]], device='cuda:0')\n",
      "Predicted class:  0\n",
      "Predicted label:  less-suspicious\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = \"training-args-openai-community/after-train-11-8-2025-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# our GPT-generated sus news, which I edited a bit for flair\n",
    "gpt_sus = \"\"\"\n",
    "A shocking leak late last night claims that top U.S. lawmakers secretly met aboard a private jet to strike hidden energy deals with foreign powers.\n",
    "How can they DO such a thing? Do they think the American people are stupid?\n",
    "Sources say the hush-hush gathering was “arranged in total darkness,” with participants leaving phones behind to avoid tracking.\n",
    "Eyewitnesses describe convoys arriving at a secluded airstrip outside D.C.\n",
    "While no official statements have been made, online forums are exploding with claims of a massive political cover-up ... Subject: Government News on February 3 2016\n",
    "\"\"\"\n",
    "\n",
    "# a known less_sus from training data\n",
    "less_sus = \"\"\"\n",
    "Former CIA director John Brennan on Friday criticized as “disgraceful” President Donald Trump’s efforts to play down U.S. intelligence agencies’ assessment that Russia meddled in the 2016 U.S. election.\n",
    "Trump’s administration has been dogged by investigations into allegations of Russian interference in last year’s U.S. presidential election and possible ties with his campaign team.\n",
    "Speaking one day before his first meeting with Russian President Vladimir Putin in Hamburg earlier this month, Trum ... Subject: politicsNews on July 22, 2017 \n",
    "\"\"\"\n",
    "\n",
    "# a known more_sus from training data\n",
    "more_sus = \"\"\"\n",
    "How did this man come to OWN this store?\n",
    "There is no information on much about this fraudster except that he stole from Americans and is getting just a slap on the wrist.\n",
    "It s no wonder these people are repeat offenders!A hispanic store owner testified in court through a Spanish translator (Why no English?) that he swindled the American tax payers out of millions via food stamp fraud.\n",
    "Eduardo Leonardo plead guilty to stealing from the USDA by illegally exchanging over a million dollars of food s ... Subject: Government News on Jun 19, 2017\n",
    "\"\"\"\n",
    "\n",
    "# from a FB post\n",
    "facebook_news = \"\"\"\n",
    " BREAKING: ILLINOIS HOUSE DEMOCRATS PUSH FIRST-IN-THE-WORLD WEALTH TAX, STREAMING, CONCERT TAXES IN $1.5 BILLION TAX HIKE\n",
    "The proposal includes:\n",
    "* A 4.95% tax on unrealized capital gains for individuals with more than $1 billion in assets.\n",
    "* A 7% amusement tax on streaming services such as Netflix and Spotify.\n",
    "* Increasing the sales tax in Cook County on certain food items by 0.25 percentage points.\n",
    "* A $5 surcharge on tickets for large concerts and performances.\n",
    "* Expanded speed camera enforcement in suburban areas to generate additional transit funding ... Subject: Government News on Nov 8\n",
    "\"\"\"\n",
    "\n",
    "# https://www.kff.org/affordable-care-act/poll-support-for-extending-the-expiring-enhanced-aca-tax-credits-remains-high-but-dips-among-republicans-and-maga-supporters-as-shutdown-continues-and-partisanship-takes-hold/\n",
    "kff_news = \"\"\"\n",
    "As the government shutdown continues, public support remains high for extending the enhanced ACA tax credits set to expire at the end of the year, with three quarters (74%) of the public in favor of extending them, a new KFF Health Tracking Poll finds.\n",
    "The expiring tax credits are a central issue in the ongoing Congressional budget standoff, as Democrats want the tax credits extended as part of a budget deal while Republicans want to reopen the government before negotiating over an extension.\n",
    "Without the enhanced tax credits, ACA Marketplace enrollees who benefit from them would on average have to pay more than twice as much out of pocket in premiums next year ... Subject: Government News on Nov 8\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "demo_text = {'less_sus' : less_sus,\n",
    "             'more_sus' : more_sus,\n",
    "             'facebook_news' : facebook_news,\n",
    "             'gpt_sus' : gpt_sus,\n",
    "             'kff_news' : kff_news}\n",
    "\n",
    "\n",
    "# to add to the UMAP later...\n",
    "df_demo = pd.DataFrame(\n",
    "    {'text' : [\n",
    "    gpt_sus,\n",
    "    facebook_news,\n",
    "    kff_news\n",
    "    ],\n",
    "    'label' : ['more-suspicious', 'more-suspicious', 'less-suspicious'],\n",
    "    'split' : ['test', 'test', 'test']\n",
    "})\n",
    "\n",
    "\n",
    "#### ADD YOUR TEXT HERE (or go through all demos) ####\n",
    "\n",
    "# text = gpt_sus\n",
    "\n",
    "for source, demo in demo_text.items():\n",
    "    print(source)\n",
    "    text = demo\n",
    "\n",
    "\n",
    "    ############################\n",
    "\n",
    "    # token text to prepare input; pt is pytorch tensor; and we truncate and we pad\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "    # run the model...\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        print(\"Probabilities: \",torch.nn.functional.softmax(logits, dim=-1))    # grab probabilities\n",
    "        predicted = torch.argmax(logits, dim=-1)                                   \n",
    "        predicted_id = predicted.item() # we use this id to get the actual text label\n",
    "        print(\"Predicted class: \", predicted_id)\n",
    "        \n",
    "    if hasattr(model.config, \"id2label\"):\n",
    "        print(\"Predicted label: \", model.config.id2label[predicted_id])\n",
    "    print(\"\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3b7a11",
   "metadata": {},
   "source": [
    "# UMAP Representation of Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c4c479",
   "metadata": {},
   "source": [
    "#### Embeddings with fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "753da5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this block before running UMAP to have results for our fine-tuned model\n",
    "# load fine-tuned model and tokenizer...\n",
    "model_path = \"training-args-openai-community/after-train-11-8-2025-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d9fd6d",
   "metadata": {},
   "source": [
    "#### Embeddings with base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cfe4314e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "785b8a95af594b35a3185ae6ef71ec07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "065843aa922a4c4aa898b8a97bbeef32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "993568889fda49ed9ea60fbbde0b6619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "204a395616584bc4840dd4af66521df9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac7603c8b4b047298d86aedd7bd7b629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f252bbe67ce44887b6a3784fb86bc856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8dcb916551443c48b3170726304704f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Execute this block only, before running UMAP, to have results for base model!!\n",
    "# base model\n",
    "model_name = \"openai-community/gpt2\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(unique_labels)\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, \n",
    "                                          local_files_only=False, \n",
    "                                          force_download=True)\n",
    "\n",
    "# Configure padding\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d074e75",
   "metadata": {},
   "source": [
    "### Compute embeddings with given model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "63d0479c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                              title  \\\n",
      "0        2619  Ex-CIA head says Trump remarks on Russia inter...   \n",
      "1       16043  YOU WON’T BELIEVE HIS PUNISHMENT! HISPANIC STO...   \n",
      "2         876  Federal Reserve governor Powell's policy views...   \n",
      "3       19963  SCOUNDREL HILLARY SUPPORTER STARTS “TrumpLeaks...   \n",
      "4       10783  NANCY PELOSI ARROGANTLY DISMISSES Questions on...   \n",
      "\n",
      "                                                text          subject  \\\n",
      "0  Former CIA director John Brennan on Friday cri...     politicsNews   \n",
      "1  How did this man come to OWN this store? There...  Government News   \n",
      "2  President Donald Trump on Thursday tapped Fede...     politicsNews   \n",
      "3  Hillary Clinton ally David Brock is offering t...        left-news   \n",
      "4  Pleading ignorance is a perfect ploy for Nancy...         politics   \n",
      "\n",
      "                date  label  \n",
      "0     July 22, 2017       1  \n",
      "1       Jun 19, 2017      0  \n",
      "2  November 2, 2017       1  \n",
      "3       Sep 17, 2016      0  \n",
      "4       May 26, 2017      0  \n",
      "Former CIA director John Brennan on Friday criticized as “disgraceful” President Donald Trump’s efforts to play down U.S. intelligence agencies’ assessment that Russia meddled in the 2016 U.S. election. Trump’s administration has been dogged by investigations into allegations of Russian interference in last year’s U.S. presidential election and possible ties with his campaign team. Speaking one day before his first meeting with Russian President Vladimir Putin in Hamburg earlier this month, Trump said he suspected Russian interference in the election but that no one knows for sure. “These types of comments are just disgraceful ... and the person who said them should be ashamed of himself,” said Brennan, CIA chief under former President Barack Obama, at the Aspen Security Forum. Special Counsel Robert Mueller and several U.S. congressional committees are investigating whether Russia interfered in the election and colluded with Trump’s campaign to try to swing the race in his favor over Democratic rival Hillary Clinton.  Those probes are focused almost exclusively on Moscow’s actions, lawmakers and intelligence officials have said, and no evidence has surfaced publicly implicating other countries. Moscow has denied any interference, and Trump has said that  his campaign did not collude with Russia. Brennan said he was disappointed by the president’s handling of security issues in his first six months in office. “I must say there are disappointments that I see in terms of what Mr. Trump is doing on the international stage that I think pose serious questions about how he is keeping safe our national security,” Brennan said. Speaking at the same event in Aspen, James Clapper, the former director of national intelligence under Obama, was also critical of Trump’s administration. Asked if Jared Kushner, Trump’s son-in-law and top adviser, should have his security clearance canceled for initially failing to list on a disclosure form contacts he had with Russians, Clapper said it should be suspended pending a review. “I do think the appropriate thing here is take a pause and at least suspend a clearance until you’ve had the opportunity to investigate and then decide whether the clearance should be restored or not,” Clapper told the same panel. Brennan and Clapper also criticized Trump’s remarks in a tweet earlier this year about U.S. spy agencies in which he accused them of practices reminiscent of Nazi Germany.  “That (tweet) was a terrible insulting affront not to me or John. We get paid the big bucks to take that. But I’m talking about the rank and file, the people in the trenches, men and women, the patriots in the intelligence community and that was completely inappropriate,” said Clapper.\n",
      "['politicsNews' 'Government News' 'left-news' 'politics' 'worldnews'\n",
      " 'News' 'Middle-east' 'US_News']\n",
      "                                                text            label  split\n",
      "0  Former CIA director John Brennan on Friday cri...  less-suspicious  train\n",
      "1  How did this man come to OWN this store? There...  more-suspicious  train\n",
      "2  President Donald Trump on Thursday tapped Fede...  less-suspicious  train\n",
      "3  Hillary Clinton ally David Brock is offering t...  more-suspicious  train\n",
      "4  Pleading ignorance is a perfect ploy for Nancy...  more-suspicious  train\n",
      "The last 3 rows are our demo texts!\n",
      "\n",
      "                                                     text            label  \\\n",
      "0      Former CIA director John Brennan on Friday cri...  less-suspicious   \n",
      "1      How did this man come to OWN this store? There...  more-suspicious   \n",
      "2      President Donald Trump on Thursday tapped Fede...  less-suspicious   \n",
      "3      Hillary Clinton ally David Brock is offering t...  more-suspicious   \n",
      "4      Pleading ignorance is a perfect ploy for Nancy...  more-suspicious   \n",
      "...                                                  ...              ...   \n",
      "29995  The chief executive of the U.S. Aerospace Indu...  less-suspicious   \n",
      "29996  The following are highlights of the maiden pol...  less-suspicious   \n",
      "29997  If there s one thing President Barack Obama is...  more-suspicious   \n",
      "29998  The Syrian army and its allies have taken full...  less-suspicious   \n",
      "29999  The United States will give Israel $38 billion...  less-suspicious   \n",
      "\n",
      "       split  \n",
      "0      train  \n",
      "1      train  \n",
      "2      train  \n",
      "3      train  \n",
      "4      train  \n",
      "...      ...  \n",
      "29995  train  \n",
      "29996  valid  \n",
      "29997  train  \n",
      "29998  valid  \n",
      "29999  train  \n",
      "\n",
      "[30000 rows x 3 columns]\n",
      "Initializing TextClassificationDataset...\n",
      "Checking label consistency: ['less-suspicious', 'more-suspicious']\n",
      "Checking train label maps. label2id: {'less-suspicious': 0, 'more-suspicious': 1}, id2label: {0: 'less-suspicious', 1: 'more-suspicious'}\n",
      "Initializing TextClassificationDataset...\n",
      "Checking label consistency: ['less-suspicious', 'more-suspicious']\n",
      "Checking test label maps. label2id: {'less-suspicious': 0, 'more-suspicious': 1}, id2label: {0: 'less-suspicious', 1: 'more-suspicious'}\n",
      "Initializing TextClassificationDataset...\n",
      "Checking label consistency: ['less-suspicious', 'more-suspicious']\n",
      "Checking valid label maps. label2id: {'less-suspicious': 0, 'more-suspicious': 1}, id2label: {0: 'less-suspicious', 1: 'more-suspicious'}\n",
      "GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2ForSequenceClassification\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"less-suspicious\",\n",
      "    \"1\": \"more-suspicious\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"less-suspicious\": 0,\n",
      "    \"more-suspicious\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 50256,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing embeddings: 100%|██████████| 94/94 [06:36<00:00,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (3003, 768)\n",
      "Labels shape: (3003,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "include_demo = True\n",
    "\n",
    "######    Get data    #######\n",
    "which_df = 'pulk17'\n",
    "df_complete = get_df(which_df)\n",
    "#############################\n",
    "\n",
    "if include_demo:\n",
    "    df_complete = pd.concat([df_complete, df_demo], ignore_index=True)\n",
    "    print(\"The last 3 rows are our demo texts!\\n\\n\", df_complete.head(-3))\n",
    "\n",
    "\n",
    "_, batch_size, _ = set_hyperparameters()\n",
    "\n",
    "# Here, we can test with the other dataset! So let's prepare it.\n",
    "train_loader, test_loader, valid_loader, _, _, _, train_dataset, test_dataset, valid_dataset = prepare_data(\n",
    "    df_complete,\n",
    "    tokenizer,\n",
    "    batch_size,\n",
    "    shuffle=False # IMPORTANT FOR THE UMAP LABELS FOR DEMO TO BE CORRECT\n",
    ")\n",
    "\n",
    "# Select the split for which to compute embeddings\n",
    "embed_loader = test_loader\n",
    "length = len(test_dataset)\n",
    "\n",
    "# now we compute embeddings, using batching:\n",
    "all_embeddings = []\n",
    "all_labels = []\n",
    "\n",
    "# double-check model configs\n",
    "print(model.config)\n",
    "\n",
    "for batch in tqdm(embed_loader, desc=\"Computing embeddings\"):\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    labels = batch[\"labels\"] # see how prepare_data() is defined\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,  # gives hidden states for all layers\n",
    "            return_dict=True\n",
    "            ) \n",
    "        last_hidden = outputs.hidden_states[-1]\n",
    "        batch_embeddings = last_hidden.mean(dim=1).cpu().numpy()\n",
    "\n",
    "    all_embeddings.append(batch_embeddings)\n",
    "    all_labels.extend(labels.numpy())\n",
    "\n",
    "embeddings = np.vstack(all_embeddings)\n",
    "labels = np.array(all_labels)\n",
    "\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n",
    "print(\"Labels shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af60e11",
   "metadata": {},
   "source": [
    "### UMAP transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b5fc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2ForSequenceClassification\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"less-suspicious\",\n",
      "    \"1\": \"more-suspicious\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"less-suspicious\": 0,\n",
      "    \"more-suspicious\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 50256,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\keske\\anaconda3\\envs\\huggingface-py3.10\\lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# double-check model configs as a sanity check\n",
    "print(model.config)\n",
    "\n",
    "# reduce dimensions of our embeddings with umap, to two dimension (umap1 and umap2)\n",
    "reducer = umap.UMAP(n_components=2,\n",
    "                    random_state=42,\n",
    "                    n_neighbors=int(0.05 * length),\n",
    "                    metric='cosine',\n",
    "                    min_dist = 0.1\n",
    ")\n",
    "\n",
    "reduced = reducer.fit_transform(embeddings)  # shape [num_samples, 2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b8411d",
   "metadata": {},
   "source": [
    "### UMAP visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c4c2c279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2ForSequenceClassification\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"less-suspicious\",\n",
      "    \"1\": \"more-suspicious\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"less-suspicious\": 0,\n",
      "    \"more-suspicious\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 50256,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "gpt_sus\n",
      "facebook_news\n",
      "kff_news\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# double-check model configs; a good sanity check, lol.\n",
    "print(model.config)\n",
    "\n",
    "\n",
    "\n",
    "# make the plot!!\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "# Class 0; should be the 'more-suspicious' class, i.e., more sensational \n",
    "plt.scatter(\n",
    "    reduced[labels==0, 0], # our mask is train_labels==0, so here we select x-coords with our mask for class 0 \n",
    "    reduced[labels==0, 1], # ... select y-coords with our mask for class 1\n",
    "    c='red',\n",
    "    label='Class 0 := less-suspicious (less sensational)',\n",
    "    alpha=0.6\n",
    ")\n",
    "\n",
    "# Class 1\n",
    "plt.scatter(\n",
    "    reduced[labels==1, 0], # do the same for class 1\n",
    "    reduced[labels==1, 1],\n",
    "    c='blue',\n",
    "    label='Class 1 := more-suspicious (more sensational)',\n",
    "    alpha=0.6\n",
    ")\n",
    "\n",
    "\n",
    "if include_demo:\n",
    "    custom_tags = [\"gpt_sus\",\"facebook_news\",\"kff_news\"]\n",
    "    # Annotate last 3 custom samples\n",
    "    for i, tag in enumerate(custom_tags):\n",
    "        idx = -len(custom_tags) + i  # e.g. -3, -2, -1\n",
    "        print(tag)\n",
    "        x, y = reduced[idx, 0], reduced[idx, 1]\n",
    "        plt.scatter(x, y, c='black', s=80, edgecolors='white', zorder=3)\n",
    "        plt.text(\n",
    "            x, y, tag, fontsize=9,\n",
    "            bbox=dict(facecolor='white', edgecolor='black', alpha=0.7)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "plt.title(\"UMAP Visualization of Dataset Embeddings\")\n",
    "plt.xlabel(\"UMAP Dim 1\")\n",
    "plt.ylabel(\"UMAP Dim 2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43df8239",
   "metadata": {},
   "source": [
    "# Other Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7c5b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note from Preet, to be used for the simple feedforward nn models \n",
    "# import joblib\n",
    "\n",
    "# # save the memory of the model to a .pkl \n",
    "# joblib.dump(value={name_of_model}, filename='{filename}.pkl')\n",
    "\n",
    "# # load the model into memory, prevents re-training\n",
    "# loaded_model = joblib.load(filename='{filename}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e5e816",
   "metadata": {},
   "source": [
    "## Attempt to add Multi-class Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c1cacf",
   "metadata": {},
   "source": [
    "We can use the existing GPT-2 Backbone, just need to retrain the classification head, reusing the pretrained weights for the rest of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451c7910",
   "metadata": {},
   "source": [
    "#### Update Model and Tokenizer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0394bfa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2ForSequenceClassification(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "num_labels = 3 # 3 labels, 0: less sus, 1: more sus, 2: neutral\n",
    "\n",
    "# load GPT-2 backbone with fresh classification head for 3-class classification\n",
    "model_path = \"training-args-openai-community/after-train-11-8-2025-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4008bf59",
   "metadata": {},
   "source": [
    "#### Update Label Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a89a201",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\n",
    "    \"less_suspicious\":0,\n",
    "    \"neutral\":1,\n",
    "    \"more_suspicious\":2\n",
    "}\n",
    "id2label = {v: k for k, v in label2id.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53887bf",
   "metadata": {},
   "source": [
    "#### Get Confidence Scores from 2-Class Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7aeb153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                              title  \\\n",
      "0        2619  Ex-CIA head says Trump remarks on Russia inter...   \n",
      "1       16043  YOU WON’T BELIEVE HIS PUNISHMENT! HISPANIC STO...   \n",
      "2         876  Federal Reserve governor Powell's policy views...   \n",
      "3       19963  SCOUNDREL HILLARY SUPPORTER STARTS “TrumpLeaks...   \n",
      "4       10783  NANCY PELOSI ARROGANTLY DISMISSES Questions on...   \n",
      "\n",
      "                                                text          subject  \\\n",
      "0  Former CIA director John Brennan on Friday cri...     politicsNews   \n",
      "1  How did this man come to OWN this store? There...  Government News   \n",
      "2  President Donald Trump on Thursday tapped Fede...     politicsNews   \n",
      "3  Hillary Clinton ally David Brock is offering t...        left-news   \n",
      "4  Pleading ignorance is a perfect ploy for Nancy...         politics   \n",
      "\n",
      "                date  label  \n",
      "0     July 22, 2017       1  \n",
      "1       Jun 19, 2017      0  \n",
      "2  November 2, 2017       1  \n",
      "3       Sep 17, 2016      0  \n",
      "4       May 26, 2017      0  \n",
      "Former CIA director John Brennan on Friday criticized as “disgraceful” President Donald Trump’s efforts to play down U.S. intelligence agencies’ assessment that Russia meddled in the 2016 U.S. election. Trump’s administration has been dogged by investigations into allegations of Russian interference in last year’s U.S. presidential election and possible ties with his campaign team. Speaking one day before his first meeting with Russian President Vladimir Putin in Hamburg earlier this month, Trump said he suspected Russian interference in the election but that no one knows for sure. “These types of comments are just disgraceful ... and the person who said them should be ashamed of himself,” said Brennan, CIA chief under former President Barack Obama, at the Aspen Security Forum. Special Counsel Robert Mueller and several U.S. congressional committees are investigating whether Russia interfered in the election and colluded with Trump’s campaign to try to swing the race in his favor over Democratic rival Hillary Clinton.  Those probes are focused almost exclusively on Moscow’s actions, lawmakers and intelligence officials have said, and no evidence has surfaced publicly implicating other countries. Moscow has denied any interference, and Trump has said that  his campaign did not collude with Russia. Brennan said he was disappointed by the president’s handling of security issues in his first six months in office. “I must say there are disappointments that I see in terms of what Mr. Trump is doing on the international stage that I think pose serious questions about how he is keeping safe our national security,” Brennan said. Speaking at the same event in Aspen, James Clapper, the former director of national intelligence under Obama, was also critical of Trump’s administration. Asked if Jared Kushner, Trump’s son-in-law and top adviser, should have his security clearance canceled for initially failing to list on a disclosure form contacts he had with Russians, Clapper said it should be suspended pending a review. “I do think the appropriate thing here is take a pause and at least suspend a clearance until you’ve had the opportunity to investigate and then decide whether the clearance should be restored or not,” Clapper told the same panel. Brennan and Clapper also criticized Trump’s remarks in a tweet earlier this year about U.S. spy agencies in which he accused them of practices reminiscent of Nazi Germany.  “That (tweet) was a terrible insulting affront not to me or John. We get paid the big bucks to take that. But I’m talking about the rank and file, the people in the trenches, men and women, the patriots in the intelligence community and that was completely inappropriate,” said Clapper.\n",
      "['politicsNews' 'Government News' 'left-news' 'politics' 'worldnews'\n",
      " 'News' 'Middle-east' 'US_News']\n",
      "                                                text            label  split\n",
      "0  Former CIA director John Brennan on Friday cri...  less-suspicious  train\n",
      "1  How did this man come to OWN this store? There...  more-suspicious  train\n",
      "2  President Donald Trump on Thursday tapped Fede...  less-suspicious  train\n",
      "3  Hillary Clinton ally David Brock is offering t...  more-suspicious  train\n",
      "4  Pleading ignorance is a perfect ploy for Nancy...  more-suspicious  train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2541/30000 [05:42<1:01:38,  7.42it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m inputs = {k: v.to(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs.items()}\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     logits = model(**inputs).logits\n\u001b[32m     18\u001b[39m     \u001b[38;5;66;03m# shape: [prob_less, prob_more]\u001b[39;00m\n\u001b[32m     19\u001b[39m     probs = F.softmax(logits, dim=-\u001b[32m1\u001b[39m).cpu().numpy()[\u001b[32m0\u001b[39m] \u001b[38;5;66;03m# use the cpu\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\anaconda3\\envs\\nlp590\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\anaconda3\\envs\\nlp590\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\anaconda3\\envs\\nlp590\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1365\u001b[39m, in \u001b[36mGPT2ForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1345\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1346\u001b[39m \u001b[33;03minput_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\u001b[39;00m\n\u001b[32m   1347\u001b[39m \u001b[33;03m    `input_ids_length` = `sequence_length` if `past_key_values` is `None` else\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1361\u001b[39m \u001b[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m   1362\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1363\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m transformer_outputs = \u001b[38;5;28mself\u001b[39m.transformer(\n\u001b[32m   1366\u001b[39m     input_ids,\n\u001b[32m   1367\u001b[39m     past_key_values=past_key_values,\n\u001b[32m   1368\u001b[39m     attention_mask=attention_mask,\n\u001b[32m   1369\u001b[39m     token_type_ids=token_type_ids,\n\u001b[32m   1370\u001b[39m     position_ids=position_ids,\n\u001b[32m   1371\u001b[39m     head_mask=head_mask,\n\u001b[32m   1372\u001b[39m     inputs_embeds=inputs_embeds,\n\u001b[32m   1373\u001b[39m     use_cache=use_cache,\n\u001b[32m   1374\u001b[39m     output_attentions=output_attentions,\n\u001b[32m   1375\u001b[39m     output_hidden_states=output_hidden_states,\n\u001b[32m   1376\u001b[39m     return_dict=return_dict,\n\u001b[32m   1377\u001b[39m )\n\u001b[32m   1378\u001b[39m hidden_states = transformer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1379\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.score(hidden_states)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\anaconda3\\envs\\nlp590\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\anaconda3\\envs\\nlp590\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\anaconda3\\envs\\nlp590\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:925\u001b[39m, in \u001b[36mGPT2Model.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, cache_position, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    923\u001b[39m     all_hidden_states = all_hidden_states + (hidden_states,)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m outputs = block(\n\u001b[32m    926\u001b[39m     hidden_states,\n\u001b[32m    927\u001b[39m     past_key_values \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.gradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    928\u001b[39m     cache_position,\n\u001b[32m    929\u001b[39m     causal_mask,\n\u001b[32m    930\u001b[39m     head_mask[i],\n\u001b[32m    931\u001b[39m     encoder_hidden_states,  \u001b[38;5;66;03m# as a positional argument for gradient checkpointing\u001b[39;00m\n\u001b[32m    932\u001b[39m     encoder_attention_mask=encoder_attention_mask,\n\u001b[32m    933\u001b[39m     use_cache=use_cache,\n\u001b[32m    934\u001b[39m     output_attentions=output_attentions,\n\u001b[32m    935\u001b[39m     **kwargs,\n\u001b[32m    936\u001b[39m )\n\u001b[32m    938\u001b[39m hidden_states = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    940\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\anaconda3\\envs\\nlp590\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\anaconda3\\envs\\nlp590\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\anaconda3\\envs\\nlp590\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\anaconda3\\envs\\nlp590\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\anaconda3\\envs\\nlp590\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:413\u001b[39m, in \u001b[36mGPT2Block.forward\u001b[39m\u001b[34m(self, hidden_states, past_key_values, cache_position, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, **kwargs)\u001b[39m\n\u001b[32m    411\u001b[39m residual = hidden_states\n\u001b[32m    412\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.ln_1(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m413\u001b[39m attn_output, self_attn_weights = \u001b[38;5;28mself\u001b[39m.attn(\n\u001b[32m    414\u001b[39m     hidden_states,\n\u001b[32m    415\u001b[39m     past_key_values=past_key_values,\n\u001b[32m    416\u001b[39m     cache_position=cache_position,\n\u001b[32m    417\u001b[39m     attention_mask=attention_mask,\n\u001b[32m    418\u001b[39m     head_mask=head_mask,\n\u001b[32m    419\u001b[39m     use_cache=use_cache,\n\u001b[32m    420\u001b[39m     output_attentions=output_attentions,\n\u001b[32m    421\u001b[39m     **kwargs,\n\u001b[32m    422\u001b[39m )\n\u001b[32m    423\u001b[39m \u001b[38;5;66;03m# residual connection\u001b[39;00m\n\u001b[32m    424\u001b[39m hidden_states = attn_output + residual\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\anaconda3\\envs\\nlp590\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\anaconda3\\envs\\nlp590\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\anaconda3\\envs\\nlp590\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\anaconda3\\envs\\nlp590\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:326\u001b[39m, in \u001b[36mGPT2Attention.forward\u001b[39m\u001b[34m(self, hidden_states, past_key_values, cache_position, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, **kwargs)\u001b[39m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m    322\u001b[39m     past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_cross_attention \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_updated\n\u001b[32m    323\u001b[39m ):\n\u001b[32m    324\u001b[39m     \u001b[38;5;66;03m# save all key/value_layer to cache to be re-used for fast auto-regressive generation\u001b[39;00m\n\u001b[32m    325\u001b[39m     cache_position = cache_position \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     key_states, value_states = curr_past_key_value.update(\n\u001b[32m    327\u001b[39m         key_states, value_states, \u001b[38;5;28mself\u001b[39m.layer_idx, {\u001b[33m\"\u001b[39m\u001b[33mcache_position\u001b[39m\u001b[33m\"\u001b[39m: cache_position}\n\u001b[32m    328\u001b[39m     )\n\u001b[32m    329\u001b[39m     \u001b[38;5;66;03m# set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\u001b[39;00m\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_cross_attention:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\anaconda3\\envs\\nlp590\\Lib\\site-packages\\transformers\\cache_utils.py:776\u001b[39m, in \u001b[36mCache.update\u001b[39m\u001b[34m(self, key_states, value_states, layer_idx, cache_kwargs)\u001b[39m\n\u001b[32m    773\u001b[39m     torch.cuda.default_stream(key_states.device).wait_stream(\u001b[38;5;28mself\u001b[39m.prefetch_stream)\n\u001b[32m    774\u001b[39m     \u001b[38;5;28mself\u001b[39m.prefetch(layer_idx + \u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.only_non_sliding)\n\u001b[32m--> \u001b[39m\u001b[32m776\u001b[39m keys, values = \u001b[38;5;28mself\u001b[39m.layers[layer_idx].update(key_states, value_states, cache_kwargs)\n\u001b[32m    778\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.offloading:\n\u001b[32m    779\u001b[39m     \u001b[38;5;28mself\u001b[39m.offload(layer_idx, \u001b[38;5;28mself\u001b[39m.only_non_sliding)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\preet\\anaconda3\\envs\\nlp590\\Lib\\site-packages\\transformers\\cache_utils.py:120\u001b[39m, in \u001b[36mDynamicLayer.update\u001b[39m\u001b[34m(self, key_states, value_states, cache_kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m     \u001b[38;5;28mself\u001b[39m.lazy_initialization(key_states)\n\u001b[32m    119\u001b[39m \u001b[38;5;28mself\u001b[39m.keys = torch.cat([\u001b[38;5;28mself\u001b[39m.keys, key_states], dim=-\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[38;5;28mself\u001b[39m.values = torch.cat([\u001b[38;5;28mself\u001b[39m.values, value_states], dim=-\u001b[32m2\u001b[39m)\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.keys, \u001b[38;5;28mself\u001b[39m.values\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "# load the 2-class model, tokenizer\n",
    "model.eval()\n",
    "\n",
    "which_df = 'pulk17'\n",
    "\n",
    "df = get_df(which_df)\n",
    "confidences = []\n",
    "for text in tqdm(df['text']): \n",
    "    inputs = tokenizer(text,\n",
    "                       return_tensors='pt',\n",
    "                       truncation=True,\n",
    "                       max_length=128\n",
    "                       )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        # shape: [prob_less, prob_more]\n",
    "        probs = F.softmax(logits, dim=-1).cpu().numpy()[0] # use the cpu\n",
    "        confidence = float(probs[1]) # probability that it is more suspicious \n",
    "        confidences.append(confidence)\n",
    "\n",
    "df['confidence'] = confidences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbf46f1",
   "metadata": {},
   "source": [
    "#### Prepare Dataset with 3 Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58523291",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      7\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m1\u001b[39m \u001b[38;5;66;03m# neutral if > 0.45 & < 0.55\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# assumes that we have a df with 'confidence' column,\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# if not, \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m] = df.apply(relabel_for_multiclass, axis=\u001b[32m1\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "def relabel_for_multiclass(row, threshold_low=0.45, threshold_high=0.55):\n",
    "    if row['confidence'] < threshold_low:\n",
    "        return 0 # less suspicious if < 0.45\n",
    "    elif row['confidence'] < threshold_high:\n",
    "        return 2 # more suspicious if > 0.45\n",
    "    else:\n",
    "        return 1 # neutral if > 0.45 & < 0.55\n",
    "\n",
    "# assumes that we have a df with 'confidence' column,\n",
    "# if not, \n",
    "df['label'] = df.apply(relabel_for_multiclass, axis=1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815b35ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp590",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
